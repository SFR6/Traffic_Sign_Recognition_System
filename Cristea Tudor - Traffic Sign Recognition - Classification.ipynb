{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Methods for computing the features used for classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def hog_compute_gradients(image):\n",
    "\n",
    "    magnitude_x = np.zeros_like(image, dtype=np.float32)\n",
    "    magnitude_y = np.zeros_like(image, dtype=np.float32)\n",
    "    \n",
    "    for i in range(1, image.shape[0] - 1):\n",
    "        for j in range(1, image.shape[1] - 1):\n",
    "            magnitude_x[i, j] = image[i, j + 1].astype(np.float32) - image[i, j - 1].astype(np.float32)\n",
    "            magnitude_y[i, j] = image[i - 1, j].astype(np.float32) - image[i + 1, j].astype(np.float32)\n",
    "    \n",
    "    magnitude = np.sqrt(magnitude_x ** 2 + magnitude_y ** 2)\n",
    "    orientation = np.arctan2(magnitude_y, magnitude_x)\n",
    "    orientation[orientation > 0] *= 180 / np.pi\n",
    "    orientation[orientation < 0] = (orientation[orientation < 0] + np.pi) * 180 / np.pi\n",
    "    \n",
    "    return magnitude, orientation\n",
    "\n",
    "def hog_compute_histograms(magnitude, orientation, cell_size=(8, 8), bins=9):\n",
    "\n",
    "    hist = np.zeros(shape=(magnitude.shape[0] // cell_size[0], magnitude.shape[1] // cell_size[1], bins))\n",
    "    step_size = 180 // bins\n",
    "    \n",
    "    for i in range(hist.shape[0]):\n",
    "        for j in range(hist.shape[1]):\n",
    "            cell_magnitude = magnitude[i * cell_size[0]:(i + 1) * cell_size[0], j * cell_size[1]:(j + 1) * cell_size[1]]\n",
    "            cell_orientation = orientation[i * cell_size[0]:(i + 1) * cell_size[0], j * cell_size[1]:(j + 1) * cell_size[1]]\n",
    "            \n",
    "            cell_hist = np.zeros(bins)\n",
    "            for ii in range(cell_magnitude.shape[0]):\n",
    "                for jj in range(cell_magnitude.shape[1]):\n",
    "                    \n",
    "                    bin_index = math.floor(cell_orientation[ii, jj] / step_size - 0.5)\n",
    "                    \n",
    "                    value1 = cell_magnitude[ii, jj] * (cell_orientation[ii, jj] / step_size - 0.5)\n",
    "                    center = step_size * (bin_index + 0.5)\n",
    "                    value2 = cell_magnitude[ii, jj] * ((cell_orientation[ii, jj] - center) / step_size)\n",
    "                    \n",
    "                    cell_hist[bin_index] += value1\n",
    "                    cell_hist[(bin_index + 1) % bins] += value2\n",
    "            \n",
    "            hist[i, j] = cell_hist\n",
    "    \n",
    "    return hist\n",
    "\n",
    "def hog_normalize_histograms(hist, block_size=(2, 2), epsilon=1e-5):\n",
    "\n",
    "    normalized_hist = np.zeros(shape=(hist.shape[0] - block_size[0] + 1, hist.shape[1] - block_size[1] + 1, block_size[0], block_size[1], hist.shape[2]))\n",
    "    \n",
    "    for i in range(normalized_hist.shape[0]):\n",
    "        for j in range(normalized_hist.shape[1]):\n",
    "            block_hist = hist[i:i + block_size[0], j:j + block_size[1]]\n",
    "            block_norm = np.sqrt(np.sum(block_hist ** 2) + epsilon)\n",
    "            normalized_hist[i, j] = block_hist / block_norm\n",
    "    \n",
    "    return normalized_hist\n",
    "\n",
    "def hog(image):\n",
    "    magnitude, orientation = hog_compute_gradients(image)\n",
    "    hist = hog_compute_histograms(magnitude, orientation)\n",
    "    normalized_hist = hog_normalize_histograms(hist)\n",
    "    return normalized_hist.flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def lbp_compute_matrix(image):\n",
    "    height, width = image.shape\n",
    "    lbp_matrix = np.zeros((height, width), dtype=np.int32)\n",
    "\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            center = image[i, j].astype(np.int32)\n",
    "\n",
    "            east = 0\n",
    "            if j + 1 < width and image[i, j + 1].astype(np.int32) >= center:\n",
    "                east = 1\n",
    "            \n",
    "            north_east = 0\n",
    "            if i - 1 >= 0 and j + 1 < width and image[i - 1, j + 1].astype(np.int32) >= center: \n",
    "                north_east = 1\n",
    "\n",
    "            north = 0\n",
    "            if i - 1 >= 0 and image[i - 1, j].astype(np.int32) >= center:\n",
    "                north = 1\n",
    "\n",
    "            north_west = 0\n",
    "            if i - 1 >= 0 and j - 1 >= 0 and image[i - 1, j - 1].astype(np.int32) >= center:\n",
    "                north_west = 1\n",
    "\n",
    "            west = 0\n",
    "            if j - 1 >= 0 and image[i, j - 1].astype(np.int32) >= center:\n",
    "                west = 1\n",
    "\n",
    "            south_west = 0\n",
    "            if i + 1 < height and j - 1 >= 0 and image[i + 1, j - 1].astype(np.int32) >= center:\n",
    "                south_west = 1\n",
    "\n",
    "            south = 0\n",
    "            if i + 1 < height and image[i + 1, j].astype(np.int32) >= center:\n",
    "                south = 1\n",
    "\n",
    "            south_east = 0\n",
    "            if i + 1 < height and j + 1 < width and image[i + 1, j + 1].astype(np.int32) >= center:\n",
    "                south_east = 1\n",
    "\n",
    "            lbp_matrix[i, j] = east + 2 * north_east + 4 * north + 8 * north_west + 16 * west + 32 * south_west + 64 * south + 128 * south_east\n",
    "    return lbp_matrix\n",
    "\n",
    "def lbp_compute_histogram(lbp_matrix):\n",
    "    height, width = lbp_matrix.shape\n",
    "    lbp_hist = np.zeros(256, dtype=np.int32)\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            lbp_hist[lbp_matrix[i, j]] += 1\n",
    "    return lbp_hist\n",
    "\n",
    "def normalize_histogram(lbp_hist, M):\n",
    "    lbp_norm_hist = np.zeros(256, dtype=np.float32)\n",
    "    for i in range(256):\n",
    "        lbp_norm_hist[i] = lbp_hist[i] / M\n",
    "    return lbp_norm_hist\n",
    "\n",
    "def lbp(image, cell_size=(16, 16), bins=256, normalize=True):\n",
    "\n",
    "    height, width = image.shape\n",
    "    hist = np.zeros(shape=(height // cell_size[0], width // cell_size[1], bins))\n",
    "    \n",
    "    for i in range(hist.shape[0]):\n",
    "        for j in range(hist.shape[1]):\n",
    "            cell_image = image[i * cell_size[0]:(i + 1) * cell_size[0], j * cell_size[1]:(j + 1) * cell_size[1]]\n",
    "            \n",
    "            lbp_matrix = lbp_compute_matrix(cell_image)\n",
    "            hist[i, j] = lbp_compute_histogram(lbp_matrix)\n",
    "            if normalize:\n",
    "                hist[i, j] = normalize_histogram(hist[i, j], height * width)\n",
    "    \n",
    "    return hist.flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def histogram_3_channels(src, normalize=True):\n",
    "    height, width = src.shape[:2]\n",
    "    hist_blue = np.zeros(256, dtype=np.int32)\n",
    "    hist_green = np.zeros(256, dtype=np.int32)\n",
    "    hist_red = np.zeros(256, dtype=np.int32)\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            hist_blue[src[i, j, 0]] += 1\n",
    "            hist_green[src[i, j, 1]] += 1\n",
    "            hist_red[src[i, j, 2]] += 1\n",
    "    if normalize:\n",
    "        hist_blue = normalize_histogram(hist_blue, height * width)\n",
    "        hist_green = normalize_histogram(hist_green, height * width)\n",
    "        hist_red = normalize_histogram(hist_red, height * width)\n",
    "    final_hist = hist_blue + hist_green + hist_red\n",
    "    return final_hist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rgb_2_gray(src):\n",
    "    height, width = src.shape[:2]\n",
    "    dst = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            dst[i, j] = 0.299 * src[i, j, 0] + 0.587 * src[i, j, 1] + 0.114 * src[i, j, 2]\n",
    "\n",
    "    return dst\n",
    "\n",
    "def preprocess_image(img):\n",
    "    height, width = img.shape[:2]\n",
    "    gray_img = rgb_2_gray(img)\n",
    "\n",
    "    resized_img = img\n",
    "    if width * height < 64 * 128:\n",
    "        resized_img = cv2.resize(img, (64, 128), interpolation = cv2.INTER_CUBIC)\n",
    "    elif width * height > 64 * 128:\n",
    "        resized_img = cv2.resize(img, (64, 128), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    resized_gray_img = gray_img\n",
    "    if width * height < 64 * 128:\n",
    "        resized_gray_img = cv2.resize(gray_img, (64, 128), interpolation = cv2.INTER_CUBIC)\n",
    "    elif width * height > 64 * 128:\n",
    "        resized_gray_img = cv2.resize(gray_img, (64, 128), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    return resized_img, resized_gray_img\n",
    "\n",
    "def create_features(resized_img, resized_gray_img):\n",
    "    hog_val = hog(resized_gray_img)\n",
    "    lbp_val = lbp(resized_gray_img)\n",
    "    histogram_3_channels_val = histogram_3_channels(resized_img)\n",
    "    return hog_val, lbp_val, histogram_3_channels_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Computing and saving (on disk) the features for the training and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv('../../datasets/archive/Meta.csv')\n",
    "\n",
    "for cls in meta_data[\"ClassId\"]:\n",
    "    path = \"../../datasets/archive/Train/{0}/\".format(cls)\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        if os.path.isfile(path + file):\n",
    "            train_image = cv2.imread(path + file, cv2.IMREAD_COLOR)\n",
    "            file_name = os.path.splitext(file)[0]\n",
    "            new_path = path + \"features/\" + file_name\n",
    "            if not os.path.exists(new_path):\n",
    "                os.makedirs(new_path)\n",
    "            new_path = new_path + \"/\"\n",
    "            resized_img, resized_gray_img = preprocess_image(train_image)\n",
    "            hog_val, lbp_val, histogram_3_channels_val = create_features(resized_img, resized_gray_img)\n",
    "            hog_path = new_path + \"hog_\" + file_name + \".npy\"\n",
    "            lbp_path = new_path + \"lbp_\" + file_name + \".npy\"\n",
    "            hist_path = new_path + \"hist_\" + file_name + \".npy\"\n",
    "            np.save(hog_path, hog_val)\n",
    "            np.save(lbp_path, lbp_val)\n",
    "            np.save(hist_path, histogram_3_channels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Computing and saving (on disk) the features for the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_test = pd.read_csv(\"../../datasets/archive/Test2.csv\")\n",
    "labels = meta_data_test['Path'].to_numpy()\n",
    "\n",
    "for label in labels:\n",
    "    test_image = cv2.imread('../../datasets/archive/' + label, cv2.IMREAD_COLOR)\n",
    "    file_name = os.path.splitext(os.path.basename(label))[0]\n",
    "    numeric_part = file_name.split('/')[-1]\n",
    "    new_path = \"../../datasets/archive/Test/features/\" + numeric_part\n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path)\n",
    "    new_path = new_path + \"/\"\n",
    "    resized_img, resized_gray_img = preprocess_image(test_image)\n",
    "    histogram_3_channels_val = create_features(resized_img, resized_gray_img)\n",
    "    hog_val, lbp_val, histogram_3_channels_val = create_features(resized_img, resized_gray_img)\n",
    "    hog_path = new_path + \"hog_\" + numeric_part + \".npy\"\n",
    "    lbp_path = new_path + \"lbp_\" + numeric_part + \".npy\"\n",
    "    hist_path = new_path + \"hist_\" + numeric_part + \".npy\"\n",
    "    np.save(hog_path, hog_val)\n",
    "    np.save(lbp_path, lbp_val)\n",
    "    np.save(hist_path, histogram_3_channels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Computing and saving (on disk) the features for MY test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_test_2 = pd.read_csv(\"../../datasets/archive/MyTest.csv\")\n",
    "initial_path = \"../../datasets/archive/MyTest/result_images/\"\n",
    "images_paths = meta_data_test_2[\"Path\"]\n",
    "\n",
    "for image_path in images_paths:\n",
    "    file_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    file_name_without_extension = file_name.split('/')[-1]\n",
    "    folder_path = initial_path + file_name_without_extension + \"/\"\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        if os.path.isfile(folder_path + file):\n",
    "            test_image = cv2.imread(folder_path + file, cv2.IMREAD_COLOR)\n",
    "            subfile_name = os.path.splitext(os.path.basename(folder_path + file))[0]\n",
    "            subfile_name_without_extension = subfile_name.split('/')[-1]\n",
    "            new_path = folder_path + \"features/\" + subfile_name_without_extension\n",
    "            if not os.path.exists(new_path):\n",
    "                os.makedirs(new_path)\n",
    "            new_path = new_path + \"/\"\n",
    "            resized_img, resized_gray_img = preprocess_image(test_image)\n",
    "            hog_val, lbp_val, histogram_3_channels_val = create_features(resized_img, resized_gray_img)\n",
    "            hog_path = new_path + \"hog_\" + subfile_name_without_extension + \".npy\"\n",
    "            lbp_path = new_path + \"lbp_\" + subfile_name_without_extension + \".npy\"\n",
    "            hist_path = new_path + \"hist_\" + subfile_name_without_extension + \".npy\"\n",
    "            np.save(hog_path, hog_val)\n",
    "            np.save(lbp_path, lbp_val)\n",
    "            np.save(hist_path, histogram_3_channels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading (from disk) the features that were computed for the training and validation sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv('../../datasets/archive/Meta.csv')\n",
    "\n",
    "train_labels = []\n",
    "\n",
    "hogs = []\n",
    "lbps = []\n",
    "hists = []\n",
    "\n",
    "for cls in meta_data[\"ClassId\"]:\n",
    "    path = \"../../datasets/archive/Train/{0}/\".format(cls)\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        if os.path.isfile(path + file):\n",
    "            file_name = os.path.splitext(file)[0]\n",
    "            new_path = path + \"features/\" + file_name + \"/\"\n",
    "            hog_path = new_path + \"hog_\" + file_name + \".npy\"\n",
    "            lbp_path = new_path + \"lbp_\" + file_name + \".npy\"\n",
    "            hist_path = new_path + \"hist_\" + file_name + \".npy\"\n",
    "            hogs.append(np.load(hog_path))\n",
    "            lbps.append(np.load(lbp_path))\n",
    "            hists.append(np.load(hist_path))\n",
    "            train_labels.append(cls)\n",
    "\n",
    "data = np.concatenate((np.array(hogs), np.array(lbps), np.array(hists)), axis=1)\n",
    "labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Performing the actual split of the initial datset into training and validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.1, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementation of the KNN Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier:\n",
    "    def __init__(self, k, original_labels):\n",
    "        self.k = k\n",
    "        self.original_labels = original_labels\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        distances = self.compute_distances(X_test)\n",
    "        \n",
    "        nearest_neighbors = np.argsort(distances, axis=1)[:, :self.k]\n",
    "        nearest_labels = self.y_train[nearest_neighbors]\n",
    "        \n",
    "        y_pred = np.array([np.bincount(labels).argmax() for labels in nearest_labels])\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def predict_proba(self, X_test):\n",
    "        distances = self.compute_distances(X_test)\n",
    "        \n",
    "        nearest_neighbors = np.argsort(distances, axis=1)[:, :self.k]\n",
    "        nearest_labels = self.y_train[nearest_neighbors]\n",
    "        \n",
    "        n_samples = X_test.shape[0]\n",
    "        n_classes = len(self.original_labels)\n",
    "        y_proba = np.zeros((n_samples, n_classes))\n",
    "        \n",
    "        for i, labels in enumerate(nearest_labels):\n",
    "            counts = np.zeros(n_classes)\n",
    "            for j, label in enumerate(self.original_labels):\n",
    "                counts[j] = np.count_nonzero(labels == label)\n",
    "            y_proba[i] = counts / self.k\n",
    "        \n",
    "        return y_proba\n",
    "    \n",
    "    def compute_distances(self, X_test):\n",
    "        num_test = X_test.shape[0]\n",
    "        num_train = self.X_train.shape[0]\n",
    "        \n",
    "        dot_product = np.dot(X_test, self.X_train.T)\n",
    "        X_test_squared = np.sum(np.square(X_test), axis=1).reshape(num_test, 1)\n",
    "        X_train_squared = np.sum(np.square(self.X_train), axis=1).reshape(1, num_train)\n",
    "        \n",
    "        distances = np.sqrt(X_test_squared + X_train_squared - 2 * dot_product)\n",
    "        return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **\"Fitting\" the model and assessing its performance on the validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNNClassifier(k=17, original_labels=meta_data[\"ClassId\"])\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.954728002920774\n",
      "Precision: [0.9047619  0.94144144 0.90666667 0.84397163 0.97474747 0.91935484\n",
      " 0.93055556 0.96453901 0.98639456 0.98484848 0.9952381  1.\n",
      " 0.97435897 0.93650794 1.         1.         0.90909091 0.97435897\n",
      " 0.95       0.91666667 0.94444444 0.92592593 0.97777778 1.\n",
      " 0.97435897 0.80952381]\n",
      "Recall: [1.         0.96759259 0.94883721 0.91538462 0.91904762 0.83823529\n",
      " 0.90540541 0.92517007 0.99315068 0.94890511 1.         0.99082569\n",
      " 1.         1.         1.         1.         1.         0.92682927\n",
      " 0.93442623 1.         0.98076923 1.         0.95652174 1.\n",
      " 1.         1.        ]\n",
      "F1 Score: [0.95       0.9543379  0.92727273 0.87822878 0.94607843 0.87692308\n",
      " 0.91780822 0.94444444 0.98976109 0.96654275 0.99761337 0.99539171\n",
      " 0.98701299 0.96721311 1.         1.         0.95238095 0.95\n",
      " 0.94214876 0.95652174 0.96226415 0.96153846 0.96703297 1.\n",
      " 0.98701299 0.89473684]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(y_pred, y_val)))\n",
    "print(\"Precision: \" + str(metrics.precision_score(y_pred, y_val, average=None)))\n",
    "print(\"Recall: \" + str(metrics.recall_score(y_pred, y_val, average=None)))\n",
    "print(\"F1 Score: \" + str(metrics.f1_score(y_pred, y_val, average=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading (from disk) the features that were computed for the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta_data = pd.read_csv(\"../../datasets/archive/Test2.csv\")\n",
    "labels_test = test_meta_data['Path'].to_numpy()\n",
    "\n",
    "hogs_test = []\n",
    "lbps_test = []\n",
    "hists_test = []\n",
    "\n",
    "for label in labels_test:\n",
    "    file_name = os.path.splitext(os.path.basename(label))[0]\n",
    "    numeric_part = file_name.split('/')[-1]\n",
    "    new_path = \"../../datasets/archive/Test/features/\" + numeric_part + \"/\"\n",
    "    hog_path = new_path + \"hog_\" + numeric_part + \".npy\"\n",
    "    lbp_path = new_path + \"lbp_\" + numeric_part + \".npy\"\n",
    "    hist_path = new_path + \"hist_\" + numeric_part + \".npy\"\n",
    "    hogs_test.append(np.load(hog_path))\n",
    "    lbps_test.append(np.load(lbp_path))\n",
    "    hists_test.append(np.load(hist_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assessing the performance of the model on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tudor Cristea\\AppData\\Local\\Temp\\ipykernel_10352\\1510802343.py:46: RuntimeWarning: invalid value encountered in sqrt\n",
      "  distances = np.sqrt(X_test_squared + X_train_squared - 2 * dot_product)\n"
     ]
    }
   ],
   "source": [
    "X_test = np.concatenate((np.array(hogs_test), np.array(lbps_test), np.array(hists_test)), axis=1)\n",
    "y_test = test_meta_data['ClassId'].values\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7912162162162162\n",
      "Precision: [0.51666667 0.57361111 0.684      0.57777778 0.89545455 0.76507937\n",
      " 0.67555556 0.79777778 0.9        0.8        1.         0.99583333\n",
      " 0.93333333 0.85714286 0.96666667 0.89166667 0.76666667 0.66666667\n",
      " 0.66666667 0.78333333 0.54666667 0.67777778 0.34666667 0.93333333\n",
      " 0.83333333 0.41666667]\n",
      "Recall: [0.93939394 0.83434343 0.62637363 0.67357513 0.80189959 0.52334419\n",
      " 0.73429952 0.69980507 0.91525424 0.78504673 0.98995696 1.\n",
      " 1.         0.96774194 0.96666667 1.         0.66346154 0.55555556\n",
      " 0.53097345 0.78333333 0.58992806 0.92424242 0.54736842 0.95789474\n",
      " 1.         1.        ]\n",
      "F1 Score: [0.66666667 0.67983539 0.65391969 0.62200957 0.84609878 0.62153449\n",
      " 0.7037037  0.74558671 0.90756303 0.79245283 0.99495314 0.99791232\n",
      " 0.96551724 0.90909091 0.96666667 0.94273128 0.71134021 0.60606061\n",
      " 0.591133   0.78333333 0.56747405 0.78205128 0.4244898  0.94545455\n",
      " 0.90909091 0.58823529]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" + str(metrics.accuracy_score(pred, y_test)))\n",
    "print(\"Precision: \" + str(metrics.precision_score(pred, y_test, average=None)))\n",
    "print(\"Recall: \" + str(metrics.recall_score(pred, y_test, average=None)))\n",
    "print(\"F1 Score: \" + str(metrics.f1_score(pred, y_test, average=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Augmenting the training set with the validation and test sets (I can do this because I have my separate test set on which I will perform the predictions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = np.concatenate((X_train, X_val, X_test), axis=0)\n",
    "y_train_full = np.concatenate((y_train, y_val, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full = KNNClassifier(k=17, original_labels=meta_data[\"ClassId\"])\n",
    "model_full.fit(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Method for creating the final image that includes the predicted signs in the top right corner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_image(file_name_without_extension, label_pred_array, size=40):\n",
    "    initial_image_path = \"../../datasets/archive/MyTest/\" + file_name_without_extension + \".png\"\n",
    "    intitial_image = cv2.imread(initial_image_path, cv2.IMREAD_COLOR)\n",
    "    overlay_images_path = \"../../datasets/archive/Meta/\"\n",
    "    result_image_path = \"../../datasets/archive/MyTest/final_results/\"\n",
    "    height, width = intitial_image.shape[:2]\n",
    "    if size * len(label_pred_array) < height:\n",
    "        for i, label in enumerate(label_pred_array):\n",
    "            overlay_image = cv2.imread(overlay_images_path + str(label) + \".png\", cv2.IMREAD_COLOR)\n",
    "            resized_overlay_image = cv2.resize(overlay_image, (size, size), interpolation = cv2.INTER_AREA)\n",
    "            intitial_image[i * size:(i + 1) * size, width - size:width] = resized_overlay_image\n",
    "        cv2.imwrite(result_image_path + file_name_without_extension + \".png\", intitial_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading the features that were computed for MY test set and creating the final images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_list(s):\n",
    "    s = s.strip('[]')\n",
    "    if not s:\n",
    "        return []\n",
    "    return list(map(int, s.split(';')))\n",
    "\n",
    "meta_data_test_2 = pd.read_csv(\"../../datasets/archive/MyTest.csv\")\n",
    "initial_path = \"../../datasets/archive/MyTest/result_images/\"\n",
    "images_paths = meta_data_test_2[\"Path\"]\n",
    "label_paths = meta_data_test_2[\"ClassId\"]\n",
    "label_paths_arrays = [convert_to_list(label) for label in label_paths]\n",
    "\n",
    "proba_threshold = 0.85\n",
    "final_pred = []\n",
    "\n",
    "for images_path, label_path_array in zip(images_paths, label_paths_arrays):\n",
    "    hogs_my_test = []\n",
    "    lbps_my_test = []\n",
    "    hists_my_test = []\n",
    "\n",
    "    file_name = os.path.splitext(os.path.basename(images_path))[0]\n",
    "    file_name_without_extension = file_name.split('/')[-1]\n",
    "    folder_path = initial_path + file_name_without_extension + \"/\"\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    for file in files:\n",
    "        if os.path.isfile(folder_path + file):\n",
    "            subfile_name = os.path.splitext(os.path.basename(folder_path + file))[0]\n",
    "            subfile_name_without_extension = subfile_name.split('/')[-1]\n",
    "            new_path = folder_path + \"features/\" + subfile_name_without_extension + \"/\"\n",
    "\n",
    "            hog_path = new_path + \"hog_\" + subfile_name_without_extension + \".npy\"\n",
    "            lbp_path = new_path + \"lbp_\" + subfile_name_without_extension + \".npy\"\n",
    "            hist_path = new_path + \"hist_\" + subfile_name_without_extension + \".npy\"\n",
    "            hogs_my_test.append(np.load(hog_path))\n",
    "            lbps_my_test.append(np.load(lbp_path))\n",
    "            hists_my_test.append(np.load(hist_path))\n",
    "\n",
    "    label_pred_array = []\n",
    "    if hogs_my_test != []:\n",
    "        X_my_test = np.concatenate((np.array(hogs_my_test), np.array(lbps_my_test), np.array(hists_my_test)), axis=1)\n",
    "        pred_my_test = model_full.predict(X_my_test)\n",
    "        pred_proba_my_test = model_full.predict_proba(X_my_test)\n",
    "        for pred, pred_proba in zip(pred_my_test, pred_proba_my_test):\n",
    "            max_pred_proba = np.max(pred_proba)\n",
    "            if max_pred_proba > proba_threshold and pred not in label_pred_array:\n",
    "                label_pred_array.append(pred)\n",
    "    final_pred.append(label_pred_array)\n",
    "    create_final_image(file_name_without_extension, label_pred_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assessing the performance of the model on MY test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1275820170109356\n",
      "Precision: 0.2513973849657943\n",
      "Recall: 0.09932880125903382\n",
      "F1 Score: 0.08323461031418082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tudor Cristea\\Documents\\IP\\project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Tudor Cristea\\Documents\\IP\\project\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "mlb.fit(final_pred + label_paths_arrays)\n",
    "\n",
    "final_pred_binary = mlb.transform(final_pred)\n",
    "label_paths_binary = mlb.transform(label_paths_arrays)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(label_paths_binary, final_pred_binary)))\n",
    "print(\"Precision: \" + str(metrics.precision_score(label_paths_binary, final_pred_binary, average='macro')))\n",
    "print(\"Recall: \" + str(metrics.recall_score(label_paths_binary, final_pred_binary, average='macro')))\n",
    "print(\"F1 Score: \" + str(metrics.f1_score(label_paths_binary, final_pred_binary, average='macro')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
